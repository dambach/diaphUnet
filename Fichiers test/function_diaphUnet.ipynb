{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"function_diaphUnet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMKbpFskSceU70yiegL5sCU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZKzz15Mj19rf"},"source":["import h5py\n","import os                                 # pour manipuler et aller chercher les fichiers en local\n","import numpy as np                        # pour effectuer des opérations sur les array/tableaux\n","import scipy.io as sio                    # pour transformer les dictionnaires .mat en type numpy.ndarray\n","\n","def matlab_to_numpy(path): \n","  chemin = os.path.abspath(path)\n","  dossier = [img for img in os.listdir(chemin) if os.path.isfile(os.path.join(chemin, img))] # noms des fichiers\n","  # sauvegarde en numpy\n","  I = len(dossier)\n","  data = np.empty(I, dtype = object)\n","  for i in range(I):\n","    try:\n","      image_mat = h5py.File(os.path.join(chemin, dossier[i]), 'r')\n","      data[i] = np.array(image_mat['BigData'])\n","      data[i] = data[i].transpose(3, 2, 1, 0)\n","    except:\n","      image_mat = sio.loadmat(os.path.join(chemin, dossier[i]))\n","      data[i] = image_mat['Data'][0][0][0]\n","      N, M, T = data[i].shape[0], data[i].shape[1], data[i].shape[2]\n","      data[i] = data[i].reshape(1, N, M, T)\n","  np.save('/content/drive/MyDrive/Colab Notebooks/Data numpy/bmode.npy', data)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYQVjxiDv8CS"},"source":["import numpy as np\n","import torch                              # pour construire le modèle de réseaux de neurones\n","from torch.autograd import Variable\n","def numpy_to_tensor(path):\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  chemin = os.path.abspath(path)\n","  data = np.load(chemin, allow_pickle=True)\n","  for i in range(len(data)):\n","    if data[i].max() > 1:\n","      if data[i].ndim > 3:\n","        B, N, M, T = data[i].shape\n","        m = int(M/2)\n","        data[i] = data[i][:,:,m,:]\n","        data[i] = Variable(torch.from_numpy(data[i]/255.0).reshape(data[i].shape[0], 1, N, T)).type(torch.FloatTensor)\n","      else:\n","        B, N, M = data[i].shape\n","        tensor = Variable(torch.from_numpy(data[i]/255.0).reshape(data[i].shape[0], 1, N, M)).type(torch.FloatTensor)\n","    else:\n","      if data[i].ndim > 3:\n","        B, N, M, T = data[i].shape\n","        m = int(M/2)\n","        data[i] = data[i][:,:,m,:]\n","        data[i] = Variable(torch.from_numpy(data[i]).reshape(data[i].shape[0], 1, N, T)).type(torch.FloatTensor)\n","      else:\n","        B, N, M = data[i].shape\n","        data[i] = Variable(torch.from_numpy(data[i]).reshape(data[i].shape[0], 1, N, M)).type(torch.FloatTensor)\n","  torch.save(data, '/content/drive/MyDrive/Colab Notebooks/Data tensor/mmode_tensor.pt')\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXvwviQprKVA"},"source":["import torch                              # pour construire le modèle de réseaux de neurones\n","\n","def ConvUnet(K_in, K_out, ker = 3, pad = 1):\n","  conv = torch.nn.Sequential(\n","      torch.nn.Conv2d(K_in, K_out, kernel_size = ker, stride = 1, \n","                      padding = pad, bias = False),\n","      torch.nn.BatchNorm2d(K_out),\n","      torch.nn.ReLU(),\n","      torch.nn.Conv2d(K_out, K_out, kernel_size = ker, stride = 1, \n","                      padding = pad, bias = False),\n","      torch.nn.BatchNorm2d(K_out),\n","      torch.nn.ReLU(),\n","      #torch.nn.Dropout2d(p = 0.05),\n","  )\n","  return conv\n","\n","def crop_image(img1, img2):\n","  N_img1 = img1.size(2)\n","  M_img1 = img1.size(3)\n","\n","  N_img2 = img1.size(2)\n","  M_img2 = img2.size(3)\n","\n","  N_img = N_img1 - N_img2\n","  N_img = N_img // 2\n","  M_img = M_img1 - M_img2\n","  M_img = M_img // 2\n","  return img1[:,:, N_img:N_img1 - N_img, M_img:M_img1 - M_img]\n","\n","class U_Net(torch.nn.Module):\n","\n","    def __init__(self):\n","        super(U_Net, self).__init__()\n","\n","        self.pool = torch.nn.MaxPool2d(2, 2)\n","        C = 8\n","\n","        #Encoder\n","        self.conv1 = ConvUnet(1, C)\n","\n","        self.conv2 = ConvUnet(C, 2*C)\n","\n","        self.conv3 = ConvUnet(2*C, 4*C)\n","\n","        self.conv4 = ConvUnet(4*C, 8*C)\n","\n","        self.conv5 = ConvUnet(8*C, 16*C)\n","\n","        self.up_conv4 = torch.nn.ConvTranspose2d(16*C, 8*C, kernel_size = 2, stride = 2)\n","        self.conv12 = ConvUnet(16*C, 8*C)\n","\n","        self.up_conv5 = torch.nn.ConvTranspose2d(8*C, 4*C, kernel_size = 2, stride = 2)\n","        self.conv13 = ConvUnet(8*C, 4*C)\n","\n","        self.up_conv6 = torch.nn.ConvTranspose2d(4*C, 2*C, kernel_size = 2, stride = 2)\n","        self.conv14 = ConvUnet(4*C, 2*C)\n","\n","        self.up_conv7 = torch.nn.ConvTranspose2d(2*C, C, kernel_size = 2, stride = 2)\n","        self.conv15 = ConvUnet(2*C, C)\n","\n","        self.conv16 = torch.nn.Conv2d(C, 1, kernel_size = 1)  \n","\n","    def forward(self, x):\n","        batch_size, Ch, N, M = x.size()\n","        C = 8\n","        \n","        # Permet de traiter des entrées de tailles quelconques\n","        k1, k2 = [], []\n","        Nb_conv = 4\n","        kernel = 2*np.ones((Nb_conv, 2), dtype=int)\n","        for i in range(Nb_conv):\n","          k1.append(int(N/2**i))\n","          k2.append(int(M/2**i))\n","          if k1[i] % 2 == 0 and k2[i] % 2 == 0:\n","            kernel[i,:] = np.array([2, 2])\n","          elif k1[i] % 2 != 0 and k2[i] % 2 == 0:\n","            kernel[i,:] = np.array([3, 2])\n","          elif k1[i] % 2 == 0 and k2[i] % 2 != 0:\n","            kernel[i,:] = np.array([2, 3])\n","          elif k1[i] % 2 != 0 and k2[i] % 2 != 0:\n","            kernel[i,:] = np.array([3, 3])\n","          else:\n","            kernel[i,:] = np.array([3, 3])\n","\n","        # Encoder\n","        x1 = self.conv1(x) # 2 CROP\n","        pool1 = self.pool(x1) # 114x150 190x150\n","\n","        x2 = self.conv2(pool1) # 2 CROP\n","        pool2 = self.pool(x2) # 57x75 95x75\n","\n","        x3 = self.conv3(pool2) # 4 CROP\n","        pool3 = self.pool(x3) # 28x37 47x37\n","\n","        x4 = self.conv4(pool3) # 4 CROP\n","        pool4 = self.pool(x4) # 14x18 23x18\n","        \n","        x5 = self.conv5(pool4) # 8 CROP\n","\n","        self.up_conv4 = torch.nn.ConvTranspose2d(16*C, 8*C, kernel_size = kernel[-1,:], stride = 2).to(device)\n","        x18 = self.up_conv4(x5) # 28x37 47x37\n","        y4 = crop_image(x4, x18)\n","        x19 = torch.cat((y4, x18), dim = 1)\n","        x20 = self.conv12(x19)\n","\n","        self.up_conv5 = torch.nn.ConvTranspose2d(8*C, 4*C, kernel_size = kernel[-2,:], stride = 2).to(device)\n","        x21 = self.up_conv5(x20) # 57x75 95x75\n","        y5 = crop_image(x3, x21)\n","        x22 = torch.cat((y5, x21), dim = 1)\n","        x23 = self.conv13(x22)\n","\n","        self.up_conv6 = torch.nn.ConvTranspose2d(4*C, 2*C, kernel_size = kernel[-3,:], stride = 2).to(device)\n","        x24 = self.up_conv6(x23) # 114x150 190x150\n","        y6 = crop_image(x2, x24)\n","        x25 = torch.cat((y6, x24), dim = 1)\n","        x26 = self.conv14(x25)\n","\n","        self.up_conv7 = torch.nn.ConvTranspose2d(2*C, C, kernel_size = kernel[-4,:], stride = 2).to(device)\n","        x27 = self.up_conv7(x26) # 228x330 380x300\n","        y7 = crop_image(x1, x27)\n","        x28 = torch.cat((y7, x27), dim = 1)\n","        x29 = self.conv15(x28)\n","        \n","        out = self.conv16(x29)\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3xSrEC6rT74"},"source":["import torch\n","def initialize_parameters(model, path):\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n","  model_net = model.to(device)\n","  u_net = torch.load(path, map_location = torch.device('cpu'))\n","  u_net.eval()\n","  for param0, param in zip(u_net.parameters(), model_net.parameters()):\n","    param.data = param0.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ULj4hbfJ02a"},"source":["import cv2\n","def superposition(img, mask, alpha, beta, gam):\n","  sp1 = np.empty(len(img), dtype = object)\n","  sp2 = np.empty(len(img), dtype = object)\n","  img = img[:,0,:,:300].cpu().detach().numpy()\n","  mask = mask[:,0].cpu().detach().numpy().astype(img.dtype)\n","  linear_mask = mask\n","  for i in range(len(img)):\n","    sp1[i] = cv2.addWeighted(img[i], alpha, mask[i], beta, gam)\n","    for v in range(mask.shape[-1]):\n","      x = np.where(mask[i,:,v] == 1)[0]\n","      for u in range(mask.shape[-2]):\n","        if x.min() < u < x.max():\n","          linear_mask[i,u,v] = 0\n","    sp2[i] = cv2.addWeighted(img[i], alpha, linear_mask[i], beta, gam)\n","  return sp1, sp2"],"execution_count":null,"outputs":[]}]}